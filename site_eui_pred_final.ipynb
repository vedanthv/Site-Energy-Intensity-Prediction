{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè° Site Energy UI Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://www.clearias.com/up/energy-efficiency.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What the project is about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 : Setting Up The Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, LeaveOneGroupOut\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 : Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train_path, test_path):\n",
    "    \"\"\"Read in train and test data for a kaggle competition.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_path : str\n",
    "        The path to the training data.\n",
    "    test_path : str\n",
    "        The path to the test data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_df, test_df : pandas DataFrames\n",
    "        The train and test datasets.\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duplicates(df, drop_cols=None):\n",
    "    \"\"\"Determine and return the duplicated values in a dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe to check\n",
    "    drop_cols : str or list of str, optional\n",
    "        The columns to drop before returning duplicates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame\n",
    "        A dataframe containing the rows with duplicated values.\n",
    "    \"\"\"\n",
    "    if drop_cols is not None:\n",
    "        return df[df.drop(columns=drop_cols).duplicated()]\n",
    "    else:\n",
    "        return df[df.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, drop_cols=None):\n",
    "    \"\"\"Removes the duplicated values in a dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe to check\n",
    "    drop_cols : str or list of str, optional\n",
    "        The columns to drop before removing duplicates.\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame\n",
    "        A dataframe without duplicated.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    if drop_cols is not None:\n",
    "        df_clean = df_clean[~df_clean.drop(columns=drop_cols).duplicated()]\n",
    "    else:\n",
    "        df_clean = df_clean[~df_clean.duplicated()]\n",
    "    \n",
    "    return df_clean.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_building_data(df, groups):\n",
    "    \"\"\"Splits the WiDS 2022 dataset based on groups of facility types.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        The WiDS train or test dataset.\n",
    "    groups : dict (key: str, value: set)\n",
    "        The dictionary of facility types.  The key should be the desired group name.\n",
    "        The value should be a set containing the facility types in a given group.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dfs : dict\n",
    "        A dictionary of pandas DataFrames, one for each group.\n",
    "    \"\"\"\n",
    "    dfs = {}\n",
    "\n",
    "    for name, group in groups.items():\n",
    "        group_df = df.query(\"facility_type in @group\")\n",
    "        dfs[name] = group_df.reset_index(drop=True)\n",
    "\n",
    "    return dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scope to refactor\n",
    "def get_manual_facility_groups():\n",
    "    \"\"\"Returns the manual facility groups used in my final WiDS solution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The dictionary of facility types.\n",
    "        The key is group name (first word in facility type).\n",
    "        The value is a set containing the facility types in a given group.\n",
    "    \"\"\"\n",
    "\n",
    "    facility_groups = {\n",
    "        \"2to4_5plus_Mixed\": {\n",
    "            \"2to4_Unit_Building\",\n",
    "            \"5plus_Unit_Building\",\n",
    "            \"Mixed_Use_Predominantly_Residential\",\n",
    "        },\n",
    "        \"Commercial_Education_Mixed_Industrial_Parking\": {\n",
    "            \"Education_College_or_university\",\n",
    "            \"Education_Other_classroom\",\n",
    "            \"Education_Preschool_or_daycare\",\n",
    "            \"Education_Uncategorized\",\n",
    "            \"Commercial_Other\",\n",
    "            \"Commercial_Unknown\",\n",
    "            \"Mixed_Use_Commercial_and_Residential\",\n",
    "            \"Mixed_Use_Predominantly_Commercial\",\n",
    "            \"Industrial\",\n",
    "            \"Parking_Garage\",\n",
    "        },\n",
    "        \"Food_Grocery\": {\n",
    "            \"Food_Sales\",\n",
    "            \"Food_Service_Other\",\n",
    "            \"Food_Service_Restaurant_or_cafeteria\",\n",
    "            \"Food_Service_Uncategorized\",\n",
    "            \"Grocery_store_or_food_market\",\n",
    "        },\n",
    "        \"Health\": {\n",
    "            \"Health_Care_Inpatient\",\n",
    "            \"Health_Care_Outpatient_Clinic\",\n",
    "            \"Health_Care_Outpatient_Uncategorized\",\n",
    "            \"Health_Care_Uncategorized\",\n",
    "        },\n",
    "        \"Laboratory_Data\": {\"Laboratory\", \"Data_Center\"},\n",
    "        \"Lodging\": {\n",
    "            \"Lodging_Dormitory_or_fraternity_sorority\",\n",
    "            \"Lodging_Hotel\",\n",
    "            \"Lodging_Other\",\n",
    "            \"Lodging_Uncategorized\",\n",
    "        },\n",
    "        \"Multifamily\": {\"Multifamily_Uncategorized\"},\n",
    "        \"Office_Nursing\": {\n",
    "            \"Office_Bank_or_other_financial\",\n",
    "            \"Office_Medical_non_diagnostic\",\n",
    "            \"Office_Mixed_use\",\n",
    "            \"Office_Uncategorized\",\n",
    "            \"Nursing_Home\",\n",
    "        },\n",
    "        \"Public\": {\n",
    "            \"Public_Assembly_Drama_theater\",\n",
    "            \"Public_Assembly_Entertainment_culture\",\n",
    "            \"Public_Assembly_Library\",\n",
    "            \"Public_Assembly_Movie_Theater\",\n",
    "            \"Public_Assembly_Other\",\n",
    "            \"Public_Assembly_Recreation\",\n",
    "            \"Public_Assembly_Social_meeting\",\n",
    "            \"Public_Assembly_Stadium\",\n",
    "            \"Public_Assembly_Uncategorized\",\n",
    "            \"Public_Safety_Courthouse\",\n",
    "            \"Public_Safety_Fire_or_police_station\",\n",
    "            \"Public_Safety_Penitentiary\",\n",
    "            \"Public_Safety_Uncategorized\",\n",
    "        },\n",
    "        \"Religious\": {\"Religious_worship\"},\n",
    "        \"Retail\": {\n",
    "            \"Retail_Enclosed_mall\",\n",
    "            \"Retail_Strip_shopping_mall\",\n",
    "            \"Retail_Uncategorized\",\n",
    "            \"Retail_Vehicle_dealership_showroom\",\n",
    "        },\n",
    "        \"Warehouse_Service\": {\n",
    "            \"Warehouse_Distribution_or_Shipping_center\",\n",
    "            \"Warehouse_Nonrefrigerated\",\n",
    "            \"Warehouse_Refrigerated\",\n",
    "            \"Warehouse_Selfstorage\",\n",
    "            \"Warehouse_Uncategorized\",\n",
    "            \"Service_Drycleaning_or_Laundry\",\n",
    "            \"Service_Uncategorized\",\n",
    "            \"Service_Vehicle_service_repair_shop\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return facility_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing(df):\n",
    "    \"\"\"Counts the missing data in a dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe to count the missing data in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame\n",
    "        A summary of missing data (counts and %)\n",
    "    \"\"\"\n",
    "    missing_df = pd.DataFrame(\n",
    "        df.isna().sum().sort_values(ascending=False), columns=[\"count\"]\n",
    "    )\n",
    "    missing_df[\"percent\"] = missing_df[\"count\"] / df.shape[0]\n",
    "    return missing_df.query(\"count != 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_impute(train_df, test_df, model, ct, target, feat_names, seed):\n",
    "    \"\"\"Imputes missing data into train and test datasets with a ML model of choice.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_df : pandas DataFrame\n",
    "        The training dataset\n",
    "    test_df : pandas DataFrame\n",
    "        The test dataset\n",
    "    model : sklearn estimator\n",
    "        The machine learning model to use for imputation\n",
    "    ct : sklearn ColumnTransformer\n",
    "        The column transformer to perform on the dataset\n",
    "    target : str\n",
    "        The target variable (removed before imputation)\n",
    "    feat_names : list\n",
    "        Names of features to append to OHE features from column transformer.\n",
    "    seed : int\n",
    "        The random seed for imputation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_imp, test_imp\n",
    "        Train and test datasets with imputed values.\n",
    "    \"\"\"\n",
    "    train_imp = ct.fit_transform(train_df.drop(columns=[target]))\n",
    "    test_imp = ct.transform(test_df)\n",
    "\n",
    "    imputer = IterativeImputer(estimator=model, random_state=seed)\n",
    "\n",
    "    cols = (\n",
    "        ct.named_transformers_[\"onehotencoder\"].get_feature_names().tolist()\n",
    "        + feat_names\n",
    "    )\n",
    "\n",
    "    train_imp = pd.DataFrame(imputer.fit_transform(train_imp), columns=cols)\n",
    "    test_imp = pd.DataFrame(imputer.transform(test_imp), columns=cols)\n",
    "\n",
    "    return train_imp, test_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the columns that are imputed in the above code block are put back into the original dataset here\n",
    "def replace_columns(df, df_imp, columns):\n",
    "    \"\"\"Replace columns in a dataframe with columns from another.\n",
    "\n",
    "    Note: Meant for use with imputed datasets for WiDS 2022.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        The original dataframe.\n",
    "    df_imp : pandas DataFrame\n",
    "        The imputed dataframe.\n",
    "    columns : str or list of str\n",
    "        The columns to replace between dataframes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_replaced : pandas DataFrame\n",
    "        Dataframe with replaced columns.\n",
    "        \n",
    "    \"\"\"\n",
    "    df_replaced = df.copy()\n",
    "\n",
    "    for col in columns:\n",
    "        df_replaced[col] = df_imp[col]\n",
    "\n",
    "    return df_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scope for refactoring\n",
    "def impute_and_replace(\n",
    "    train_dfs, test_dfs, model, ct, target, feat_names, replace, seed\n",
    "):\n",
    "    \"\"\"Iteratively impute multiple dataframes.\n",
    "\n",
    "    Since I have 12 individual datasets based on the groups of facility_types, I am caling the imputer and replace functions\n",
    "    on each of those datasets in this function\n",
    "    \"\"\"\n",
    "    train_dfs_imp = {}\n",
    "    test_dfs_imp = {}\n",
    "\n",
    "    # iterative imputation\n",
    "    for (name1, train_df), (name2, test_df) in zip(train_dfs.items(), test_dfs.items()):\n",
    "        train_imp, test_imp = iterative_impute(\n",
    "            train_df, test_df, model, ct, target, feat_names, seed\n",
    "        )\n",
    "        train_dfs_imp[name1] = train_imp\n",
    "        test_dfs_imp[name1] = test_imp\n",
    "\n",
    "    # replace train columns with missing values\n",
    "    for (name1, df), (name2, imp_df) in zip(train_dfs.items(), train_dfs_imp.items()):\n",
    "        train_dfs[name1] = replace_columns(df, imp_df, replace)\n",
    "\n",
    "    # replace test columns with missing values\n",
    "    for (name1, df), (name2, imp_df) in zip(test_dfs.items(), test_dfs_imp.items()):\n",
    "        test_dfs[name1] = replace_columns(df, imp_df, replace)\n",
    "\n",
    "    return train_dfs, test_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_feature(df, feat):\n",
    "    \"\"\"Adds a boolean feature if a feature exists or now\"\"\"\n",
    "    return df[feat].notna().astype(int)\n",
    "\n",
    "# Avoids repeated use of groupbys in the notebook\n",
    "def group_by_feature(train_df, test_df, group, agg_feat, transform, name):\n",
    "    \"\"\"Adds a new aggregated feature based on a categorical variable\"\"\"\n",
    "    train_df_new = train_df.copy()\n",
    "    test_df_new = test_df.copy()\n",
    "    \n",
    "    fill_values = train_df_new.groupby(group).aggregate(transform)[agg_feat].to_dict()\n",
    "    \n",
    "    train_df_new[name] = train_df_new[group].map(fill_values)\n",
    "    test_df_new[name] = test_df_new[group].map(fill_values)\n",
    "    \n",
    "    return train_df_new, test_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_days_below_above(df):\n",
    "    \"\"\"Bins the days below and above features into 4 bins\"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    df_eng[\"freezing_days\"] = df_eng['days_below_0F'] + df_eng['days_below_10F']\n",
    "    df_eng[\"cold_days\"] = df_eng['days_below_20F'] + df_eng['days_below_30F']\n",
    "    df_eng[\"warm_days\"] = df_eng['days_above_80F'] + df_eng['days_above_90F']\n",
    "    df_eng[\"hot_days\"] = df_eng['days_above_100F'] + df_eng['days_above_110F']\n",
    "    \n",
    "    return df_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_temps(df, stat):\n",
    "    \"\"\"Add seasonal temperatures for a given statistic specified by user\"\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "\n",
    "    df_eng[f\"winter_{stat}\"] = (\n",
    "        df_eng[f\"december_{stat}\"]\n",
    "        + df_eng[f\"january_{stat}\"]\n",
    "        + df_eng[f\"february_{stat}\"]\n",
    "    ) / 3\n",
    "\n",
    "    df_eng[f\"spring_{stat}\"] = (\n",
    "        df_eng[f\"march_{stat}\"] + df_eng[f\"april_{stat}\"] + df_eng[f\"june_{stat}\"]\n",
    "    ) / 3\n",
    "\n",
    "    df_eng[f\"summer_{stat}\"] = (\n",
    "        df_eng[f\"june_{stat}\"] + df_eng[f\"july_{stat}\"] + df_eng[f\"august_{stat}\"]\n",
    "    ) / 3\n",
    "    \n",
    "    df_eng[f\"autumn_{stat}\"] = (\n",
    "        df_eng[f\"september_{stat}\"]\n",
    "        + df_eng[f\"october_{stat}\"]\n",
    "        + df_eng[f\"november_{stat}\"]\n",
    "    ) / 3\n",
    "\n",
    "    return df_eng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(train_df, test_df):\n",
    "    train_df_eng = train_df.copy()\n",
    "    test_df_eng = test_df.copy()\n",
    "\n",
    "    # whether or not a building has a fog detector, This I have added because majority of buildings do not have a fog detector\n",
    "    \n",
    "    train_df_eng[\"has_fog_detector\"] = has_feature(train_df_eng, \"days_with_fog\")\n",
    "    test_df_eng[\"has_fog_detector\"] = has_feature(test_df_eng, \"days_with_fog\")\n",
    "\n",
    "    # whether or not a building has a wind detector\n",
    "    train_df_eng[\"has_wind_detector\"] = has_feature(train_df_eng, \"max_wind_speed\")\n",
    "    test_df_eng[\"has_wind_detector\"] = has_feature(test_df_eng, \"max_wind_speed\")\n",
    "    \n",
    "    # seasonal avg temps\n",
    "    train_df_eng = seasonal_temps(train_df_eng, \"avg_temp\")\n",
    "    test_df_eng = seasonal_temps(test_df_eng, \"avg_temp\")\n",
    "    \n",
    "    # aggregate features\n",
    "    agg_feats = [\"energy_star_rating\", \"floor_area\", \"ELEVATION\"]\n",
    "\n",
    "    for agg_feat in agg_feats:\n",
    "        name = \"mean_\" + agg_feat\n",
    "        train_df_eng, test_df_eng = group_by_feature(\n",
    "            train_df_eng, test_df_eng, \"facility_type\", agg_feat, \"mean\", name\n",
    "        )\n",
    "    \n",
    "    # whether or not energy star is better than mean for facility\n",
    "    train_df_eng[\"e_star_better_than_mean\"] = (\n",
    "        train_df_eng[\"energy_star_rating\"] > train_df_eng[\"mean_energy_star_rating\"]\n",
    "    ).astype(int)\n",
    "    test_df_eng[\"e_star_better_than_mean\"] = (\n",
    "        test_df_eng[\"energy_star_rating\"] > test_df_eng[\"mean_energy_star_rating\"]\n",
    "    ).astype(int)\n",
    "    \n",
    "    # total snow and rain\n",
    "    train_df_eng[\"snow_rain_inches\"] = (\n",
    "        train_df_eng[\"snowfall_inches\"] + train_df_eng[\"precipitation_inches\"]\n",
    "    )\n",
    "    test_df_eng[\"snow_rain_inches\"] = (\n",
    "        test_df_eng[\"snowfall_inches\"] + test_df_eng[\"precipitation_inches\"]\n",
    "    )\n",
    "\n",
    "    # total degree days\n",
    "    train_df_eng[\"degree_days\"] = (\n",
    "        train_df_eng[\"cooling_degree_days\"] + train_df_eng[\"heating_degree_days\"]\n",
    "    )\n",
    "    test_df_eng[\"degree_days\"] = (\n",
    "        test_df_eng[\"cooling_degree_days\"] + test_df_eng[\"heating_degree_days\"]\n",
    "    )\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
